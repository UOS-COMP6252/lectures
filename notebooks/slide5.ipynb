{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HTsa4o4TAkhT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as vision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD,Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization     \n",
    "Given a mini-batch of tensors $x_{ci}$ of dimension (S,C,H,W) where $c$ is the channel index and $i$ collectively refers to all other dimensions. \n",
    "\n",
    "Let $N=S\\times H\\times W$. Batch normalization computes the mean and variance of the batch (per channel) according to\n",
    "  $$\n",
    "    \\begin{align*}\n",
    "    \\mu_c&=\\frac{1}{N}\\sum_{i=1}^N x_{ci}\\\\\n",
    "    \\sigma^2_c&=\\frac{1}{N}\\sum_{i=1}^N \\left(x_{ci}-\\mu_c\\right)^2\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "The normalized inputs are computed as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{x}_{ci}=\\frac{x_{ic}-\\mu_c}{\\sqrt{\\sigma^2_c+\\epsilon}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore, for each channel, the $\\hat{x}_{ci}$ have zero mean and unit variance. The output of the batch normalization layer is given by\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_{ic}=\\gamma \\hat{x}_{ic}+\\beta\n",
    "\\end{align*}\n",
    "$$\n",
    "Where $\\gamma$ and $\\beta$ are **learnable** parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "- For simplicity we consider a  tensor with a single channel\n",
    "- Recall that batch normalization is done for each channel independently\n",
    "- In the example below we create an arbitrary tensor ```a```  of size ```(2,1,22)```\n",
    "- It represents two samples, each with a single channel representing a 2x2 tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2, 2]\n",
      "[[[[ 1.  2.]\n",
      "   [ 3.  4.]]]\n",
      "\n",
      "\n",
      " [[[ 5.  6.]\n",
      "   [ 7. 18.]]]]\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[1,2],[3,4]],dtype=torch.float32).unsqueeze(0)\n",
    "y=torch.tensor([[5,6],[7,18]],dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "a=torch.stack([x,y])\n",
    "print(list(a.size()))\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manual Computation vs Normalization Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.9512, -0.7509],\n",
       "          [-0.5507, -0.3504]]],\n",
       "\n",
       "\n",
       "        [[[-0.1502,  0.0501],\n",
       "          [ 0.2503,  2.4531]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually \n",
    "var=a.var([0,2,3],unbiased=False)\n",
    "mean=a.mean([0,2,3])\n",
    "(a-mean)/torch.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.9512, -0.7509],\n",
       "          [-0.5507, -0.3504]]],\n",
       "\n",
       "\n",
       "        [[[-0.1502,  0.0501],\n",
       "          [ 0.2503,  2.4531]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using PyTorch BatchNorm2d\n",
    "with torch.no_grad():\n",
    "    norm=nn.BatchNorm2d(num_features=1)  \n",
    "    b=norm(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Network for CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure reproducibility\n",
    "seed=9 \n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "# use/not use batch normalization\n",
    "use_BN=False\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "dataset_train=vision.datasets.CIFAR10(\".\",download=True,train=True,transform=transform)\n",
    "dataset_test=vision.datasets.CIFAR10(\".\",download=True,train=False,transform=transform)\n",
    "loader_train=DataLoader(dataset_train,batch_size=64,shuffle=True,num_workers=2)\n",
    "loader_test=DataLoader(dataset_test,batch_size=512,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self,norm_layers=True):\n",
    "    super().__init__()\n",
    "    self.norm_layers=norm_layers\n",
    "    # self.norm1=nn.BatchNorm2d(32)\n",
    "    # self.norm2=nn.BatchNorm2d(32)\n",
    "    # self.norm3=nn.BatchNorm2d(64)\n",
    "    # self.norm4=nn.BatchNorm2d(64)\n",
    "    self.norm1=nn.Dropout2d(0.5)\n",
    "    self.norm2=nn.Dropout2d(0.5)\n",
    "    self.norm3=nn.Dropout2d(0.5)\n",
    "    self.norm4=nn.Dropout2d(0.5)\n",
    "    self.relu=nn.ReLU()\n",
    "    # input is (*,3,32,32)\n",
    "    self.conv1=nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3)\n",
    "    # input is (*,32,30,30)\n",
    "    self.conv2=nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3)\n",
    "    self.conv2a=nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3)\n",
    "    # input is (*,32,28,28)\n",
    "    self.pool1=nn.MaxPool2d(kernel_size=(2,2))\n",
    "    # input is (*,32,14,14)\n",
    "    self.conv3=nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3)\n",
    "    # input is (*,64,12,12)\n",
    "    self.conv4=nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3)\n",
    "    self.conv4a=nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3)\n",
    "    # input is (*,64,10,10)\n",
    "    self.pool2=nn.MaxPool2d(kernel_size=(2,2))\n",
    "    # input is (*,64,5,5)\n",
    "    self.flatten=nn.Flatten()\n",
    "    # input is (*,64x5x5)\n",
    "    self.fc1=nn.Linear(in_features=5*5*64,out_features=128)\n",
    "    self.fc2=nn.Linear(in_features=128,out_features=10)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x=self.conv1(x)\n",
    "    if self.norm_layers:\n",
    "      x=self.norm1(x)\n",
    "    x=self.relu(x)\n",
    "    x=self.conv2(x)\n",
    "    if self.norm_layers:\n",
    "      x=self.norm2(x)\n",
    "    x=self.relu(x)\n",
    "########\n",
    "    x=self.conv2a(x)\n",
    "    x=self.relu(x)\n",
    "########\n",
    "    x=self.pool1(x)\n",
    "    \n",
    "    x=self.conv3(x)\n",
    "    if self.norm_layers:\n",
    "      x=self.norm3(x)\n",
    "    x=self.relu(x)\n",
    "    x=self.conv4(x)\n",
    "    if self.norm_layers:\n",
    "      x=self.norm4(x)\n",
    "    x=self.relu(x)\n",
    "    #####\n",
    "    x=self.conv4a(x)\n",
    "    x=self.relu(x)\n",
    "\n",
    "    ###\n",
    "    x=self.pool2(x)\n",
    "    \n",
    "    x=self.flatten(x)\n",
    "    x=self.fc1(x)\n",
    "    x=self.relu(x)\n",
    "    x=self.fc2(x)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model,batch,loss_fn):\n",
    "    imgs,labels=batch\n",
    "    imgs=imgs.cuda()\n",
    "    labels=labels.cuda()\n",
    "    outputs=model(imgs)\n",
    "    _,pred=torch.max(outputs,dim=1)\n",
    "    acc=torch.sum(pred==labels).item()\n",
    "    loss=loss_fn(outputs,labels)\n",
    "    return loss,torch.tensor(acc/len(labels))\n",
    "\n",
    "@torch.no_grad() \n",
    "def evaluate(model,loader,loss_fn):\n",
    "    model.eval()\n",
    "    # crit is a list of pairs of tensors\n",
    "    crit=[accuracy(model,batch,loss_fn) for batch in loader]\n",
    "    crit=torch.tensor(crit)\n",
    "    m=crit.mean(dim=0)\n",
    "    loss=m[0]\n",
    "    acc=m[1]\n",
    "    return loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Net(norm_layers=use_BN).cuda()\n",
    "optimizer=Adam(model.parameters())\n",
    "#optimizer=SGD(model.parameters(),lr=0.5)\n",
    "loss_fn=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "# To display tensorboard inside the notebook\n",
    "%load_ext tensorboard\n",
    "current=datetime.datetime.now()\n",
    "log_dir = 'logs/tensorboard/' + ('with BN-' if use_BN else 'withou BN-')+current.strftime(\"%c\")\n",
    "writer=SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self,patience=4,tolerance=0):\n",
    "        self.patience=patience\n",
    "        self.tolerance=tolerance\n",
    "        self.min_loss=float('inf')\n",
    "        self.count=0\n",
    "    def __call__(self,loss):\n",
    "        if loss<self.min_loss:\n",
    "            self.count=0\n",
    "            self.min_loss=loss\n",
    "            return False\n",
    "        elif loss>self.min_loss+self.tolerance:\n",
    "            self.count+=1\n",
    "            if self.count>self.patience:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]:   0%|          | 0/782 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x576 and 1600x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m imgs\u001b[39m=\u001b[39mimgs\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     12\u001b[0m labels\u001b[39m=\u001b[39mlabels\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> 13\u001b[0m outputs\u001b[39m=\u001b[39mmodel(imgs)\n\u001b[1;32m     14\u001b[0m loss\u001b[39m=\u001b[39mloss_fn(outputs,labels)\n\u001b[1;32m     15\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [7], line 65\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool2(x)\n\u001b[1;32m     64\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x)\n\u001b[0;32m---> 65\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[1;32m     66\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     67\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x576 and 1600x128)"
     ]
    }
   ],
   "source": [
    "trigger=True\n",
    "es=EarlyStopping()\n",
    "from tqdm import tqdm\n",
    "for epoch in range(epochs):\n",
    "  loop=tqdm(loader_train)\n",
    "  loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "  epoch_loss=0.\n",
    "  model.train()\n",
    "  for (imgs,labels) in loop:\n",
    "    optimizer.zero_grad()\n",
    "    imgs=imgs.cuda()\n",
    "    labels=labels.cuda()\n",
    "    outputs=model(imgs)\n",
    "    loss=loss_fn(outputs,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch_loss=0.9*epoch_loss+0.1*loss.item()\n",
    "    loop.set_postfix(loss=epoch_loss)\n",
    "   \n",
    "  t_loss,t_acc=evaluate(model,loader_train,loss_fn)\n",
    "  v_loss,v_acc=evaluate(model,loader_test,loss_fn)\n",
    "  writer.add_scalar(\"Epoch loss\",epoch_loss,epoch)\n",
    "  writer.add_scalars(\"loss\",{'train':t_loss,'valid':v_loss},epoch)\n",
    "  writer.add_scalars(\"acc\",{'train':t_acc,'valid':v_acc},epoch)\n",
    "  if es(v_loss) and trigger:\n",
    "  #   break\n",
    "     print(\"At epoch={} we should stop. Validation accuracy={}\".format(epoch,v_acc))\n",
    "     trigger=False\n",
    "writer.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics\n",
    "from torchmetrics import ConfusionMatrix\n",
    "conmat=ConfusionMatrix(num_classes=10)\n",
    "conmat=conmat.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=0\n",
    "correct=0\n",
    "for data in loader_test:\n",
    "  imgs,labels=data\n",
    "  imgs=imgs.cuda()\n",
    "  labels=labels.cuda()\n",
    "  outputs=model(imgs)\n",
    "  # the second return value is the index of the max i.e. argmax\n",
    "  _,predicted=torch.max(outputs.data,1)\n",
    "  correct+=(predicted==labels).sum()\n",
    "  total+=labels.size()[0]\n",
    "  conmat.update(predicted,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "x=conmat.compute().cpu().numpy()\n",
    "plt.figure(figsize=(10,7))\n",
    "sb.heatmap(x,xticklabels=dataset_train.classes,yticklabels=dataset_train.classes,annot=True,fmt=\".0f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The rows are the actual images and the columns are the prediction (How can you check?)\n",
    "- While the prediction accuracy is good albeit not impressive\n",
    "- From the confusion matrix we find justifications for the inaccuracies\n",
    "- For example\n",
    "    - most of the incorrect classifications of automobiles were classified as trucks\n",
    "    - most of the incorrect classifications of cats/dogs were classified as dogs/cats\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
