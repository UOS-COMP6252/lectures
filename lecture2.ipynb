{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture2\n",
    "Learning outcomes:\n",
    "1. Be able to use the chain rule to compute the gradient for composite functions\n",
    "1. Be able to construct the computational graph for any sequence of computation\n",
    "1. Understand \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    " We mentioned in lecture 1 that PyTorch can compute the gradient to any function. To accomplish this, PyTorch uses three concepts:\n",
    "\n",
    " \n",
    " 1. Computational graph\n",
    " 1. \"Database\" of derivatives to **primitive** functions\n",
    " 1. Chain rule\n",
    " We explain the three concepts  with an example. Consider the following computations:\n",
    "\n",
    " $a=2$\n",
    " \n",
    " $b=4$\n",
    " \n",
    " $c=a+b$\n",
    " \n",
    " $d=\\log(a)*\\log(b)$\n",
    "\n",
    " $e=c*d$\n",
    "\n",
    " Where $\\log(x)$ is the log base two. \n",
    " \n",
    " When a gradient is needed, in this case $\\frac{\\partial e}{\\partial a}$ and $\\frac{\\partial e}{\\partial b}$, PyTorch builds a **computational graph** to perform the operations as shown in the figure below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Comp-graph-1](comp-graph1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every **primitive** operation, PyTorch \"knows\" its derivative. For example, PyTorch has the following rules stored and can look them up when needed.\n",
    "\n",
    "1. $\\frac{\\partial\\log(x)}{\\partial x}=\\frac{1}{x*\\ln(2)}$\n",
    "1. $\\frac{\\partial (x*y)}{\\partial x}=y$\n",
    "1. $\\frac{\\partial (x*y)}{\\partial y}=x$\n",
    "1. $\\frac{\\partial (x+y)}{\\partial x}=1$\n",
    "1. $\\frac{\\partial (x+y)}{\\partial y}=1$\n",
    "\n",
    "\n",
    "Using that information, PyTorch adds, at each step, auxillary nodes to be able to compute the gradient as shown in the figure below.\n",
    "\n",
    "**NOTE**: Actually the node $d$ involves multiple primitive operations than what is shown (see later) but for now we keep it as it is for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Comp-graph-2](comp-graph2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Suppose that we need to compute the gradient of $e$ with respect to $a$ and $b$. Having the above computational graph, PyTorch uses the chain rule:\n",
    "\n",
    " $$\\frac{\\partial e}{\\partial a}=\\frac{\\partial e}{\\partial c}\\frac{\\partial c}{\\partial a}+\\frac{\\partial e}{\\partial d}\\frac{\\partial d}{\\partial a}$$\n",
    " $$\\frac{\\partial e}{\\partial b}=\\frac{\\partial e}{\\partial c}\\frac{\\partial c}{\\partial b}+\\frac{\\partial e}{\\partial d}\\frac{\\partial d}{\\partial b}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To \"Walk backward\" in the computational graph to compute the gradients. This means that the value at every node is the product of its parent value with the auxillary node (the contributions of multiple parents are added), as shown in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![backprop](backprop.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, in order to keep the graph simple, we glossed over the fact that $d=\\log(a)*\\log(b)$ involves **multiple primitive operations**. Below is the breakdown of how PyTorch actually computes the gradient of $d$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![backprop](comp-graph3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyTorch code corresponding to the example is shown below. Note \n",
    "1. For each **leaf** tensor ```X```, the gradient is stored in ```X.grad```\n",
    "1. The values for **non-leaf** node ```Y``` is **not** saved, unless ```Y.retain_grad()``` is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a=torch.tensor(2.,requires_grad=True)\n",
    "b=torch.tensor(4.,requires_grad=True)\n",
    "c=a+b\n",
    "d=torch.log(a)*torch.log(b)\n",
    "e=c*d\n",
    "e.backward()\n",
    "print(a.grad,b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
